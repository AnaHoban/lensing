{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('WANDB_NOTEBOOK_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('WANDB_NOTEBOOK_NAME')\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import (ZScaleInterval, ImageNormalize)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from functions import plot_loss_curves\n",
    "%load_ext autoreload\n",
    "from func_job1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_dir = os.path.expandvars(\"$SCRATCH\") + \"/\"\n",
    "image_dir = \"/home/anahoban/projects/rrg-kyi/astro/cfis/W3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tile ids\n",
    "tile_list = open(image_dir + \"tiles.list\", \"r\")\n",
    "\n",
    "# Only use tiles with all five channels\n",
    "tile_list = open(image_dir + \"tiles.list\", \"r\")\n",
    "tile_ids = []\n",
    "\n",
    "for tile in tile_list:\n",
    "    tile = tile[:-1] # Remove new line character\n",
    "    channels = tile.split(\" \")\n",
    "    if len(channels) == 5: # Order is u,g,r,i,z\n",
    "        tile_ids.append(channels[0][5:12]) # XXX.XXX id\n",
    "tile_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= h5py.File(cutout_dir + \"test.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(cutout_dir + \"test.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 #128\n",
    "CUTOUT_SIZE = 64\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import (ZScaleInterval, ImageNormalize)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from functions import plot_loss_curves\n",
    "%load_ext autoreload\n",
    "from func_job2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cutout_dir + 'tiles_unbalanced.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data  = list(reader)\n",
    "f.close()\n",
    "\n",
    "tile_ids = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">treasured-galaxy-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mizar/autoencoder%20tests\" target=\"_blank\">https://wandb.ai/mizar/autoencoder%20tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mizar/autoencoder%20tests/runs/2ob8zzr5\" target=\"_blank\">https://wandb.ai/mizar/autoencoder%20tests/runs/2ob8zzr5</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/anahoban/stronglens/Code/Batch Loading/Currently used/wandb/run-20210607_151457-2ob8zzr5</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start a W&B run\n",
    "wandb.init(project='autoencoder tests', entity='mizar')\n",
    "\n",
    "# 2. Save model inputs and hyperparameters\n",
    "config = wandb.config\n",
    "#config.learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING PREP##\n",
    "BATCH_SIZE  = 256 \n",
    "CUTOUT_SIZE = 64\n",
    "N_EPOCHS    = 2 \n",
    "\n",
    "# tiles for val and training\n",
    "train_indices = range(1)\n",
    "val_indices = [2]\n",
    "\n",
    "#autosave\n",
    "model_checkpoint_file = \"../Models/job8.h5\"\n",
    "model_checkpoint_callback = ModelCheckpoint(model_checkpoint_file, monitor='val_loss', mode='min',verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "#training\n",
    "bands = 2\n",
    "autoencoder_cfis = create_autoencoder2((CUTOUT_SIZE, CUTOUT_SIZE, bands*2)) #last is the number of channels\n",
    "autoencoder_cfis.compile(optimizer=\"adam\", loss=MSE_with_uncertainty)\n",
    "\n",
    "(autoencoder_cfis, history_cfis) = train_autoencoder(hf, tile_ids, autoencoder_cfis, train_indices,  val_indices, batch_size=BATCH_SIZE, cutout_size=CUTOUT_SIZE, n_epochs= N_EPOCHS, all_callbacks = [model_checkpoint_callback], bands=\"cfis\")\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(hf, tile_ids, model, train_indices, val_indices, n_epochs, batch_size, cutout_size, all_callbacks = None, bands=\"all\"):\n",
    "    n_cutouts_train = 0\n",
    "    for i in train_indices:\n",
    "        img_group = hf.get(tile_ids[i] + \"/IMAGES\")   \n",
    "        n_cutouts_train += len(img_group)\n",
    "    \n",
    "    n_cutouts_val = 0    \n",
    "    for i in val_indices:\n",
    "        img_group = hf.get(tile_ids[i] + \"/IMAGES\")        \n",
    "        n_cutouts_val += len(img_group)\n",
    "    \n",
    "    train_steps = n_cutouts_train // batch_size\n",
    "    val_steps = n_cutouts_val // batch_size\n",
    "    \n",
    "    history = model.fit(get_cutouts(hf, tile_ids, train_indices, batch_size, cutout_size, bands), \n",
    "                        epochs=n_epochs, steps_per_epoch=train_steps, \n",
    "                        validation_data=get_cutouts(hf, tile_ids, val_indices, batch_size, cutout_size, bands), \n",
    "                        validation_steps=val_steps, callbacks= all_callbacks)\n",
    "    \n",
    "    # log metrics using wandb.log\n",
    "    wandb.log({'epochs': len(history),\n",
    "                   'loss': history['loss'][-1], \n",
    "                   'val_loss': history['val_loss'][-1]})\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto(hf, tile_ids, model, train_indices, val_indices, n_epochs, batch_size, cutout_size, all_callbacks = None, bands=\"all\"):\n",
    "    n_cutouts_train = 0\n",
    "    for i in train_indices:\n",
    "        img_group = hf.get(tile_ids[i] + \"/IMAGES\")   \n",
    "        n_cutouts_train += len(img_group)\n",
    "    \n",
    "    n_cutouts_val = 0    \n",
    "    for i in val_indices:\n",
    "        img_group = hf.get(tile_ids[i] + \"/IMAGES\")        \n",
    "        n_cutouts_val += len(img_group)\n",
    "    \n",
    "    train_steps = n_cutouts_train // batch_size\n",
    "    val_steps = n_cutouts_val // batch_size\n",
    "    \n",
    "    history = model.fit(get_cutouts(hf, tile_ids, train_indices, batch_size, cutout_size, bands), \n",
    "                        epochs=n_epochs, steps_per_epoch=train_steps, \n",
    "                        validation_data=get_cutouts(hf, tile_ids, val_indices, batch_size, cutout_size, bands), \n",
    "                        validation_steps=val_steps, callbacks= all_callbacks)\n",
    "    \n",
    "    # log metrics using wandb.log\n",
    "    wandb.log({'epochs': len(history),\n",
    "                   'loss': history['loss'][-1], \n",
    "                   'val_loss': history['val_loss'][-1]})\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto(hf, tile_ids, model, train_indices, val_indices, n_epochs, batch_size, cutout_size, all_callbacks = None, bands=\"all\"):\n",
    "    n_cutouts_train = 0\n",
    "    for i in train_indices:\n",
    "        img_group = hf.get(tile_ids[i] + \"/IMAGES\")   \n",
    "        n_cutouts_train += len(img_group)\n",
    "    \n",
    "    n_cutouts_val = 0    \n",
    "    for i in val_indices:\n",
    "        img_group = hf.get(tile_ids[i] + \"/IMAGES\")        \n",
    "        n_cutouts_val += len(img_group)\n",
    "    \n",
    "    train_steps = n_cutouts_train // batch_size\n",
    "    val_steps = n_cutouts_val // batch_size\n",
    "    \n",
    "    history = model.fit(get_cutouts(hf, tile_ids, train_indices, batch_size, cutout_size, bands), \n",
    "                        epochs=n_epochs, steps_per_epoch=train_steps, \n",
    "                        validation_data=get_cutouts(hf, tile_ids, val_indices, batch_size, cutout_size, bands), \n",
    "                        validation_steps=val_steps, callbacks= all_callbacks)\n",
    "    \n",
    "    # log metrics using wandb.log\n",
    "    wandb.log({'epochs': range(n_epochs),\n",
    "                   'loss': history['loss'], \n",
    "                   'val_loss': history['val_loss']})\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING PREP##\n",
    "BATCH_SIZE  = 256 \n",
    "CUTOUT_SIZE = 64\n",
    "N_EPOCHS    = 2 \n",
    "configs = {\n",
    "              \"epochs\": 2,\n",
    "              \"batch_size\": 256,\n",
    "              \"cutout_size\": 54,\n",
    "              \"architecture\": \"CNN\",\n",
    "              \"dataset\": \"test\"\n",
    "           }\n",
    "run = wandb.init(project='my-tf-integration', entity = 'mizar', config=configs)\n",
    "config = wandb.config\n",
    "\n",
    "# tiles for val and training\n",
    "train_indices = range(1)\n",
    "val_indices = [2]\n",
    "\n",
    "#autosave\n",
    "model_checkpoint_file = \"../Models/job8.h5\"\n",
    "model_checkpoint_callback = ModelCheckpoint(model_checkpoint_file, monitor='val_loss', mode='min',verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "#training\n",
    "bands = 2\n",
    "autoencoder_cfis = create_autoencoder2((CUTOUT_SIZE, CUTOUT_SIZE, bands*2)) #last is the number of channels\n",
    "autoencoder_cfis.compile(optimizer=\"adam\", loss=MSE_with_uncertainty)\n",
    "\n",
    "(autoencoder_cfis, history_cfis) = train_auto(hf, tile_ids, autoencoder_cfis, train_indices,  val_indices, batch_size=BATCH_SIZE, cutout_size=CUTOUT_SIZE, n_epochs= N_EPOCHS, all_callbacks = [model_checkpoint_callback], bands=\"cfis\")\n",
    "\n",
    "hf.close()\n",
    "\n",
    "run.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
